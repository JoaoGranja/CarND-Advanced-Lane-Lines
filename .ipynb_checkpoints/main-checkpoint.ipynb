{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "## The project is divided in the four following phases:\n",
    "\n",
    "1 - Compute the camera calibration using chessboard images\n",
    "\n",
    "2 - Build a Lane finding Pipeline for single images\n",
    "\n",
    "3 - Build a Lane finding Pipeline to a video\n",
    "\n",
    "4 - Reinforce the video Pipeline using the provided challenge videos \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T13:19:27.806418Z",
     "start_time": "2020-05-14T13:19:26.028101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing some useful packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from helper_functions import *\n",
    "import pickle\n",
    "\n",
    "#Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the output directory to store the output images\n",
    "output_path = \"output_images/camera_calibration\"\n",
    "\n",
    "# Create output_path directory if doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T13:19:31.648297Z",
     "start_time": "2020-05-14T13:19:27.811639Z"
    }
   },
   "outputs": [],
   "source": [
    "# To avoid running the calibration step everytime, in case the camera coefficients already exist, just load it\n",
    "if os.path.exists('camera_coeff.pkl'):\n",
    "    # Getting back the values:\n",
    "    with open('camera_coeff.pkl', 'rb') as f:  \n",
    "        mtx, dist = pickle.load(f)\n",
    "    \n",
    "else:  \n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(9,6,0)\n",
    "    objp = np.zeros((9*6,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for fname in images:\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(fname)\n",
    "\n",
    "        # Convert the image to gray scale\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard 54 (9*6) corners \n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "        # If found, append object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "            #cv2.imshow('img',img)\n",
    "            #cv2.waitKey(500)\n",
    "\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    # Perform the camera calibration\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None) \n",
    "\n",
    "    # Store the camera calibration coefficients for future use\n",
    "    with open('camera_coeff.pkl', 'wb') as f:  \n",
    "        pickle.dump([mtx, dist], f)\n",
    "        \n",
    "    # Perform distortion correction on chessboard images to verify the process is doing well\n",
    "\n",
    "    # Step through the list and undistort each image\n",
    "    for fname in images:\n",
    "\n",
    "        # Read the image\n",
    "        image = mpimg.imread(fname)\n",
    "\n",
    "        # Undistort the image an display it \n",
    "        undist = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "\n",
    "        # Save images on output_path Directory\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "\n",
    "        ax1.imshow(image)\n",
    "        ax1.set_title('Original Image', fontsize=30)\n",
    "\n",
    "        ax2.imshow(undist)\n",
    "        ax2.set_title('Undistorted Image', fontsize=30)\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=1, bottom=0, wspace = 0.1)\n",
    "\n",
    "\n",
    "        plt.savefig(os.path.join(output_path, \"undist_\" + os.path.basename(fname)))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Build a Lane finding Pipeline for single images\n",
    "The pipeline will be composed by the following steps:\n",
    "\n",
    "1 - Read and apply a distortion correction to the image\n",
    "\n",
    "2 - Use color transforms, gradients, etc., to create a thresholded binary image\n",
    "\n",
    "3 - Apply a perspective transform to rectify binary image (\"birds-eye view\")\n",
    "\n",
    "4 - Detect lane pixels and fit to find the lane boundary\n",
    "\n",
    "5 - Determine the curvature of the lane and vehicle position with respect to center \n",
    "\n",
    "6 - Warp the detected lane boundaries back onto the original image\n",
    "\n",
    "7 - Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the directory to store the output images\n",
    "output_path = \"output_images/test_images\"\n",
    "\n",
    "# Create output_path directory if doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transform matrix using 4 source and destination points calculated manually looking to \n",
    "# the straight line images\n",
    "\n",
    "src = np.float32([[195, 720],[1125, 720],[578, 460],[705, 460]])\n",
    "dst = np.float32([[350, 720],[950, 720],[350,0],[950,0]])\n",
    "\n",
    "# Get the transform matrix M using the src and dst points\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# Get the invert transform matrix M_inv using the src and dst points\n",
    "M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(original_image, fname, mtx, dist, M, M_inv):\n",
    "    \n",
    "    ## 1 - Apply a distortion correction to the image ##\n",
    "    undist = undistort_image(original_image, mtx, dist)\n",
    "    save_undistorted_images(output_path, fname, original_image, undist)\n",
    "\n",
    "    ## 2 - Use color transforms, gradients, etc., to create a thresholded binary image ##\n",
    "    ksize = 3 # Sobel kernel size \n",
    "\n",
    "    # Apply each of the gradient thresholding functions\n",
    "    gradx = abs_sobel_thresh(undist, orient='x', sobel_kernel=ksize, thresh=(30, 200))\n",
    "    grady = abs_sobel_thresh(undist, orient='y', sobel_kernel=ksize, thresh=(50, 200))\n",
    "    mag_binary = mag_thresh(undist, sobel_kernel=ksize, mag_thresh=(30, 200))\n",
    "    dir_binary = dir_threshold(undist, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "    # Apply each of the color thresholding functions for HLS color space\n",
    "    hls_colors_binary = hls_select(undist, s_thresh=(170, 240), l_thresh=(200, 255))\n",
    "\n",
    "    # Apply each of the color thresholding functions for HSV color space\n",
    "    hsv_colors_binary = hsv_select(undist, s_thresh=(130,255), v_thresh=(240, 255), vs_thresh=(200, 255))\n",
    "        \n",
    "    # Combine all of the thresholding binaries\n",
    "    binary_image = np.zeros_like(mag_binary)\n",
    "    binary_image[(hsv_colors_binary == 1) | (hls_colors_binary == 1) | ((mag_binary == 1) & (dir_binary == 1)) ] = 1\n",
    "    save_binary_images(output_path, fname, undist, binary_image)\n",
    "    \n",
    "    # 3 - Apply a perspective transform to rectify binary image (\"birds-eye view\") ##\n",
    "\n",
    "    # Warp the image to a top-down view\n",
    "    img_size = (undist.shape[1], undist.shape[0])\n",
    "    binary_warped = cv2.warpPerspective(binary_image, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    warped = cv2.warpPerspective(undist, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    save_warped_images(output_path, fname, original_image, warped)\n",
    "\n",
    "    ## 4 - Detect lane pixels and fit to find the lane boundary ##\n",
    "    \n",
    "    # Create a sliding window and find out which activated pixels fall into the window\n",
    "    out_img, left_fit, right_fit, left_fitx, right_fitx, ploty = fit_polynomial(binary_warped)\n",
    "\n",
    "    ## 5 - Determine the curvature of the lane and vehicle position with respect to center ##\n",
    "    \n",
    "    radius_curvature, left_radius_curvature , right_radius_curvature = measure_curvature_real(out_img.shape[0], left_fit, right_fit)\n",
    "    \n",
    "    rel_vehicle_position = measure_rel_vehicle_position(out_img.shape, left_fit, right_fit)\n",
    "\n",
    "    ## 6 - Warp the detected lane boundaries back onto the original image ##\n",
    "\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, M_inv, img_size) \n",
    "\n",
    "    ## 7 - Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position ##\n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(original_image, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    save_lane_lines_image(output_path, fname, out_img)\n",
    "\n",
    "    print(\"Radius curvature = \", radius_curvature, 'm')\n",
    "    print(\"Left Radius curvature = \", left_radius_curvature, 'm', \"Right Radius curvature = \", right_radius_curvature, 'm', )\n",
    "    print(\"Relative vehicle position with respect to the line lane center = \",rel_vehicle_position, 'm')\n",
    "\n",
    "    save_pipeline_image(output_path, fname, result, radius_curvature, rel_vehicle_position )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T13:26:17.757281Z",
     "start_time": "2020-05-14T13:26:17.693455Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Make a list of test images\n",
    "images = glob.glob('test_images/*.jpg')\n",
    "images = []\n",
    "\n",
    "# Step through the list, read the image and apply the lane finding pipeline\n",
    "for fname in images:\n",
    "\n",
    "    print(\"-----------------------\", fname, \"-----------------------\")\n",
    "    \n",
    "    # Read the image\n",
    "    img = mpimg.imread(fname)\n",
    "\n",
    "    process_image(img, fname, mtx, dist, M, M_inv)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 -  Build a Lane finding Pipeline to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T13:44:25.527191Z",
     "start_time": "2020-05-14T13:44:25.087142Z"
    }
   },
   "outputs": [],
   "source": [
    "from frame import *\n",
    "img_size = (720,1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if True:\n",
    "if False:\n",
    "    white_output = 'project_video_out.mp4'\n",
    "    clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "    white_clip = clip1.fl_image(Frame(mtx, dist, M, M_inv, img_size, 2))#.subclip(0,5)\n",
    "    %time white_clip.write_videofile(white_output, audio=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'white_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-68061e453756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{0}\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \"\"\".format(white_output))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'white_output' is not defined"
     ]
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Test the pipeline on Challenge Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radius curvature =  474.0 m\n",
      "Left Radius curvature =  628.0 m Right Radius curvature =  320.0 m\n",
      "Relative vehicle position with respect to the line lane center =  0.02 m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the transform matrix using 4 source and destination points calculated looking to the straight line image\n",
    "src = np.float32([[250, 720],[1050, 720],[605, 480],[715, 480]])\n",
    "dst = np.float32([[350, 720],[950, 720],[350,0],[950,0]])\n",
    "\n",
    "# Get the transform matrix M using the src and dst points\n",
    "M_1 = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# Get the invert transform matrix M_inv using the src and dst points\n",
    "M_inv_1 = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "fname = \"challenge_video.jpg\"\n",
    "\n",
    "# Read the image\n",
    "clip1 = VideoFileClip(\"challenge_video.mp4\")\n",
    "img = clip1.get_frame(52 / clip1.fps) # get frame by index\n",
    "img_size = img.shape\n",
    "\n",
    "process_image(img, fname, mtx, dist, M_1, M_inv_1)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if True:\n",
    "if False:\n",
    "    white_output = 'challenge_video_out.mp4'\n",
    "    clip1 = VideoFileClip(\"challenge_video.mp4\")\n",
    "    white_clip = clip1.fl_image(Frame(mtx, dist, M_1, M_inv_1, img_size,2)).subclip(0,5) #NOTE: this function expects color images!!\n",
    "    %time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'white_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-68061e453756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{0}\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \"\"\".format(white_output))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'white_output' is not defined"
     ]
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_1(original_image, fname, mtx, dist, M, M_inv):\n",
    "    \n",
    "    ## 1 - Apply a distortion correction to the image ##\n",
    "    undist = undistort_image(original_image, mtx, dist)\n",
    "    save_undistorted_images(output_path, fname, original_image, undist)\n",
    "\n",
    "    ## 2 - Use color transforms, gradients, etc., to create a thresholded binary image ##\n",
    "    ksize = 3 # Sobel kernel size \n",
    "\n",
    "    # Apply each of the gradient thresholding functions\n",
    "    gradx = abs_sobel_thresh(undist, orient='x', sobel_kernel=ksize, thresh=(30, 200))\n",
    "    grady = abs_sobel_thresh(undist, orient='y', sobel_kernel=ksize, thresh=(50, 200))\n",
    "    mag_binary = mag_thresh(undist, sobel_kernel=ksize, mag_thresh=(30, 200))\n",
    "    dir_binary = dir_threshold(undist, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "    # Apply each of the color thresholding functions for HLS color space\n",
    "    hls_colors_binary = hls_select(undist, s_thresh=(170, 240), l_thresh=(200, 255)) # l - 200\n",
    "\n",
    "    # Apply each of the color thresholding functions for HSV color space\n",
    "    hsv_colors_binary = hsv_select(undist, s_thresh=(170,240), v_thresh=(240, 255), vs_thresh=(200, 255)) # v - 200\n",
    "        \n",
    "    # Combine all of the thresholding binaries\n",
    "    binary_image = np.zeros_like(mag_binary)\n",
    "    binary_image[(hsv_colors_binary == 1) | (hls_colors_binary == 1) | ((mag_binary == 1) & (dir_binary == 1)) ] = 1\n",
    "    binary_image = hls_colors_binary\n",
    "    save_binary_images(output_path, fname, undist, binary_image)\n",
    "    \n",
    "    # 3 - Apply a perspective transform to rectify binary image (\"birds-eye view\") ##\n",
    "\n",
    "    # Warp the image to a top-down view\n",
    "    img_size = (undist.shape[1], undist.shape[0])\n",
    "    binary_warped = cv2.warpPerspective(binary_image, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    warped = cv2.warpPerspective(undist, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    save_warped_images(output_path, fname, binary_warped, warped)\n",
    "\n",
    "    ## 4 - Detect lane pixels and fit to find the lane boundary ##\n",
    "    \n",
    "    # Create a sliding window and find out which activated pixels fall into the window\n",
    "    out_img, left_fit, right_fit, left_fitx, right_fitx, ploty = fit_polynomial(binary_warped)\n",
    "\n",
    "    ## 5 - Determine the curvature of the lane and vehicle position with respect to center ##\n",
    "    \n",
    "    radius_curvature, left_radius_curvature , right_radius_curvature = measure_curvature_real(out_img.shape[0], left_fit, right_fit)\n",
    "\n",
    "    rel_vehicle_position = measure_rel_vehicle_position(out_img.shape, left_fit, right_fit)\n",
    "\n",
    "    ## 6 - Warp the detected lane boundaries back onto the original image ##\n",
    "\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, M_inv, img_size) \n",
    "\n",
    "    ## 7 - Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position ##\n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    result = cv2.addWeighted(original_image, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    save_lane_lines_image(output_path, fname, out_img)\n",
    "\n",
    "    print(\"Radius curvature = \", radius_curvature, 'm')\n",
    "    print(\"Left Radius curvature = \", left_radius_curvature, 'm', \"Right Radius curvature = \", right_radius_curvature, 'm', )\n",
    "    print(\"Relative vehicle position with respect to the line lane center = \",rel_vehicle_position, 'm')\n",
    "\n",
    "    save_pipeline_image(output_path, fname, result, radius_curvature, rel_vehicle_position )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radius curvature =  4077.0 m\n",
      "Left Radius curvature =  132.0 m Right Radius curvature =  8022.0 m\n",
      "Relative vehicle position with respect to the line lane center =  0.08 m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the transform matrix using 4 source and destination points calculated looking to the straight line image\n",
    "src = np.float32([[250, 720],[1050, 720],[605, 480],[715, 480]])\n",
    "dst = np.float32([[350, 720],[950, 720],[350,0],[950,0]])\n",
    "\n",
    "# Get the transform matrix M using the src and dst points\n",
    "M_1 = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# Get the invert transform matrix M using the src and dst points\n",
    "M_inv_1 = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "fname = \"challenge_video.jpg\"\n",
    "\n",
    "# Read the image\n",
    "clip1 = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "img = clip1.get_frame(100 / clip1.fps) # get frame by index\n",
    "img_size = img.shape\n",
    "\n",
    "process_image_1(img, fname, mtx, dist, M, M_inv)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/125 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video harder_challenge_video_out.mp4.\n",
      "Moviepy - Writing video harder_challenge_video_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  15%|█▌        | 19/125 [00:04<00:28,  3.79it/s, now=None]"
     ]
    }
   ],
   "source": [
    "white_output = 'harder_challenge_video_out.mp4'\n",
    "clip1 = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "white_clip = clip1.fl_image(Frame(mtx, dist, M, M_inv, img_size, 2, True)).subclip(0,5) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
